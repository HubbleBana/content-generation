version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: sleep-stories-ollama
    ports:
      - "11434:11434"
    volumes:
      - sleepai_volume:/root/.ollama
    runtime: nvidia  # Explicit NVIDIA runtime
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_KEEP_ALIVE=5m  # Reduced keep-alive to prevent hanging
      - OLLAMA_MAX_LOADED_MODELS=1
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    restart: unless-stopped
    networks:
      - sleep-stories-net
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:11434/api/tags || exit 1"]
      interval: 30s
      timeout: 20s
      retries: 5
      start_period: 60s  # Give more time for GPU initialization

  backend:
    build: ./backend
    container_name: sleep-stories-api
    ports:
      - "8000:8000"
    volumes:
      - sleepai_volume:/app/data
      - ./backend/logs:/app/logs
    depends_on:
      ollama:
        condition: service_started  # Changed from service_healthy to avoid blocking
    environment:
      - OLLAMA_URL=http://ollama:11434
      - DATA_PATH=/app/data
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      # Gunicorn tuning to avoid worker timeouts on long GPU-bound tasks
      - WEB_CONCURRENCY=1
      - GUNICORN_TIMEOUT=1800           # 30 minutes
      - GUNICORN_WORKER_CLASS=uvicorn.workers.UvicornWorker
      - GUNICORN_KEEPALIVE=75
      - GUNICORN_GRACEFUL_TIMEOUT=120
      # Enhanced configuration
      - SENSORY_ROTATION_ENABLED=true
      - SLEEP_TAPER_ENABLED=true
      - BEAT_PLANNING_ENABLED=true
      - MAX_CONCURRENT_MODELS=1
      - MAX_RETRIES=3
      - FALLBACK_MODEL=qwen3:8b
      - OLLAMA_CONNECT_TIMEOUT=120
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 16G
    restart: unless-stopped
    networks:
      - sleep-stories-net
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 90s

  ui:
    build: ./ui
    container_name: sleep-stories-ui-v2
    ports:
      - "7860:7860"
    volumes:
      - sleepai_volume:/app/data
      - ./ui/logs:/app/logs
    depends_on:
      - backend
    environment:
      - API_URL=http://backend:8000/api
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=7860
      - DATA_PATH=/app/data
      - GRADIO_THEME=soft
      - GRADIO_ANALYTICS_ENABLED=false
      - UI_AUTO_REFRESH=true
      - UI_MAX_HISTORY=50
      - UI_STREAMING_ENABLED=true
    restart: unless-stopped
    networks:
      - sleep-stories-net
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:7860 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 45s

networks:
  sleep-stories-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  sleepai_volume:
    external: true

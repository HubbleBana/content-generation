version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: sleep-stories-ollama
    ports:
      - "11434:11434"
    volumes:
      - sleepai_volume:/root/.ollama
    runtime: nvidia
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_MAX_LOADED_MODELS=1
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    restart: unless-stopped
    networks:
      - sleep-stories-net

  backend:
    build: ./backend
    container_name: sleep-stories-api
    ports:
      - "8000:8000"
    volumes:
      - sleepai_volume:/app/data
      - ./backend/logs:/app/logs
    depends_on:
      - ollama
    environment:
      - OLLAMA_URL=http://ollama:11434
      - DATA_PATH=/app/data
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=INFO
      - WEB_CONCURRENCY=1
      - GUNICORN_TIMEOUT=1800
      - GUNICORN_WORKER_CLASS=uvicorn.workers.UvicornWorker
      - GUNICORN_KEEPALIVE=75
      - GUNICORN_GRACEFUL_TIMEOUT=120
    restart: unless-stopped
    networks:
      - sleep-stories-net

  ui:
    build: ./ui
    container_name: sleep-stories-ui-v2
    ports:
      - "7860:7860"
    volumes:
      - sleepai_volume:/app/data
      - ./ui/logs:/app/logs
    depends_on:
      - backend
    environment:
      - API_URL=http://backend:8000/api
      - GRADIO_SERVER_NAME=0.0.0.0
      - GRADIO_SERVER_PORT=7860
      - DATA_PATH=/app/data
      - GRADIO_THEME=soft
      - GRADIO_ANALYTICS_ENABLED=false
      - UI_AUTO_REFRESH=true
      - UI_MAX_HISTORY=50
      - UI_STREAMING_ENABLED=true
    restart: unless-stopped
    networks:
      - sleep-stories-net

networks:
  sleep-stories-net:
    driver: bridge

volumes:
  sleepai_volume:
    external: true

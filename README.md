# ğŸŒ™ Sleep Stories AI - Enhanced v2.0

**Next-Generation AI-Powered Sleep Story Generator with Multi-Model Architecture**

ğŸ† **COMPLETELY REWRITTEN FRONTEND** - Now with real-time streaming, enhanced UI, and full Gradio 4.44+ compatibility!

---

## ğŸš€ **New in v2.0**

### âœ… **Frontend Completely Rewritten**
- **Real-time streaming** with Server-Sent Events
- **Working job management** with functional dropdowns
- **All parameters exposed** - complete backend control
- **Modern responsive UI** with glassmorphism design
- **Gradio 4.44+ compatible** - no more compatibility errors

### âœ… **Technical Improvements**
- **Modular architecture** with separated components
- **Enhanced API client** with SSE and fallback polling  
- **Optimized for RTX 3070Ti** with proper resource management
- **Docker improvements** with health checks and troubleshooting

### âœ… **Issues Resolved**
- **âœ… Stream updates working** - real-time progress tracking
- **âœ… Dropdown menus fixed** - job selection now functional
- **âœ… Parameter exposure complete** - all backend options available
- **âœ… Gradio compatibility** - updated for 4.44+ versions

---

## ğŸ—ï¸ **System Requirements**

### **Hardware**
- **GPU**: RTX 3070Ti (8GB VRAM) or equivalent NVIDIA GPU
- **RAM**: 16GB+ recommended
- **Storage**: 20GB+ free space for models and data

### **Software**
- **Docker** with NVIDIA runtime support
- **Docker Compose** v2.0+
- **NVIDIA Drivers** 470.x+ with CUDA support

---

## ğŸš€ **Quick Start**

### **1. Clone Repository**
```bash
git clone https://github.com/HubbleBana/content-generation.git
cd content-generation
```

### **2. Switch to Enhanced Branch**
```bash
git checkout frontend-rewrite-v2
```

### **3. Create Docker Volume**
```bash
docker volume create sleepai_volume
```

### **4. Start Services**
```bash
# Standard startup
docker-compose up --build

# Or use troubleshooting script if Ollama has issues
chmod +x scripts/fix-ollama.sh
./scripts/fix-ollama.sh
```

### **5. Access Application**
- **Frontend UI**: http://localhost:7860
- **Backend API**: http://localhost:8000/docs
- **Ollama API**: http://localhost:11434

---

## ğŸ¯ **Features Overview**

### ğŸ¨ **Story Generation**
- **Multi-model orchestration** (Generator + Reasoner + Polisher)
- **Sensory rotation** with systematic sensory cycling
- **Sleep taper** with progressive relaxation
- **TTS markers** for speech synthesis integration
- **JSON schema output** for video production

### ğŸ” **Real-time Monitoring**
- **Live progress tracking** with beat-by-beat updates
- **Job management** with attachment and resuming
- **System telemetry** with resource monitoring
- **Quality metrics** with automated assessment

### ğŸ¥ **YouTube Integration Ready**
- **Structured output** with timing and media cues
- **TTS-ready text** with pause and breath markers
- **Video segments** with visual focus suggestions
- **Export functionality** for production workflows

---

## ğŸ“Š **Architecture**

### **Frontend (Enhanced v2.0)**
```
ui/
â”œâ”€â”€ app.py                    # Main Gradio application
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ generation_panel.py   # Generation controls
â”‚   â”œâ”€â”€ monitoring_panel.py   # Real-time monitoring
â”‚   â””â”€â”€ results_panel.py      # Results and analysis
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ api_client.py         # Enhanced API client with SSE
â”‚   â””â”€â”€ helpers.py            # Utility functions
â””â”€â”€ static/
    â””â”€â”€ custom.css            # Modern UI styling
```

### **Backend (Multi-Model)**
- **FastAPI** with streaming endpoints
- **Multi-model orchestration** for quality enhancement
- **Real-time telemetry** with detailed progress tracking
- **Structured output** for video production pipelines

### **Infrastructure**
- **Docker Compose** with health checks
- **Ollama** for local LLM hosting
- **NVIDIA GPU** optimization for RTX 3070Ti
- **Volume management** for persistent data

---

## ğŸ”§ **Configuration**

### **Environment Variables**
```bash
# Core Configuration
OLLAMA_URL=http://ollama:11434
DATA_PATH=/app/data
LOG_LEVEL=INFO

# GPU Optimization
MAX_CONCURRENT_MODELS=1
OLLAMA_MAX_LOADED_MODELS=1
OLLAMA_KEEP_ALIVE=5m

# Enhanced Features
SENSORY_ROTATION_ENABLED=true
SLEEP_TAPER_ENABLED=true
BEAT_PLANNING_ENABLED=true

# UI Configuration
UI_AUTO_REFRESH=true
UI_STREAMING_ENABLED=true
GRADIO_ANALYTICS_ENABLED=false
```

### **Model Presets**
- **Quality High**: All models enabled (15-25 min generation)
- **Balanced**: Generator + Reasoner (10-15 min generation)
- **Fast**: Generator only (5-10 min generation)
- **Custom**: User-defined model configuration

---

## ğŸ› **Troubleshooting**

### **Ollama Container Issues**
```bash
# Use the automated fix script
./scripts/fix-ollama.sh

# Or debug manually
docker-compose -f docker-compose.debug.yml up ollama
```

### **Gradio Compatibility Issues**
The system is now **fully compatible with Gradio 4.44+**. All compatibility issues have been resolved:
- âœ… Queue configuration updated
- âœ… Event handlers modernized
- âœ… Component parameters cleaned

See `GRADIO_4_44_COMPATIBILITY.md` for full details.

### **Common Issues**

**Q: Stream updates not appearing**
- âœ… Fixed in v2.0 with proper SSE implementation

**Q: Job dropdown not working**
- âœ… Fixed in v2.0 with enhanced job management

**Q: Missing parameters in UI**
- âœ… Fixed in v2.0 - all backend parameters now exposed

**Q: Container hanging on startup**
- Use `./scripts/fix-ollama.sh` for automated diagnosis and fix

---

## ğŸ“ˆ **Performance**

### **RTX 3070Ti Optimized**
- **Sequential model loading** to stay within 8GB VRAM
- **Memory management** with automatic cleanup
- **Queue optimization** for GPU resource allocation

### **Generation Times**
|Configuration|Duration|Quality Score|VRAM Usage|
|-------------|--------|-------------|----------|
|Quality High|15-25 min|95%+|4.7GB peak|
|Balanced|10-15 min|90%+|4.1GB peak|
|Fast|5-10 min|85%+|4.1GB peak|

---

## ğŸ“ **Documentation**

- **Frontend Rewrite**: `FRONTEND_REWRITE_V2.md`
- **Gradio Compatibility**: `GRADIO_4_44_COMPATIBILITY.md`  
- **API Documentation**: http://localhost:8000/docs
- **Troubleshooting**: `scripts/fix-ollama.sh`

---

## ğŸ¤ **Contributing**

### **Development Setup**
```bash
# Clone and setup
git clone https://github.com/HubbleBana/content-generation.git
cd content-generation
git checkout frontend-rewrite-v2

# Install development dependencies
pip install -r ui/requirements.txt
pip install -r backend/requirements.txt

# Run in development mode
cd ui && python app.py
```

### **Pull Request Guidelines**
- **Frontend changes**: Update components in `ui/components/`
- **API changes**: Update backend in `backend/app/api/`
- **Documentation**: Update relevant `.md` files
- **Testing**: Ensure compatibility with Gradio 4.44+

---

## ğŸ“ **License**

MIT License - see `LICENSE` for details.

---

## ğŸ‘¥ **Team**

- **John**: AI Specialist & System Architecture
- **Carl**: Social Media Manager & Content Strategy  
- **Jimmy**: Frontend Expert & UI/UX (v2.0 Rewrite)

---

## ğŸ† **Status**

**âœ… PRODUCTION READY** - Enhanced v2.0

- âœ… **Real-time streaming** working
- âœ… **Job management** functional
- âœ… **All parameters** exposed
- âœ… **Gradio 4.44+** compatible
- âœ… **Docker optimized** with troubleshooting
- âœ… **RTX 3070Ti** performance tuned
- âœ… **YouTube integration** ready

**Ready for YouTube content generation! ğŸš€ğŸŒ™**

---

*Sleep Stories AI - Where Technology Meets Tranquility*
